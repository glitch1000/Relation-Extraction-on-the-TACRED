{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!pip install numpy\n",
    "!pip install torch\n",
    "!pip install scikit-learn\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Unified Relation Extraction Model\n",
    "\n",
    "\n",
    "This model integrates three sources of features:\n",
    "- A BiLSTM with attention that processes token embeddings.\n",
    "- Rule-based features computed via simple regex-based matching.\n",
    "- SVM-style features computed via a TF-IDF representation.\n",
    "\n",
    "These three representations are projected into a common space,\n",
    "concatenated, and fed to a final classifier so that all\n",
    "information is jointly learned during training.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import math\n",
    "import spacy\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from collections import Counter\n",
    "import re\n",
    "import random\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        # Model parameters\n",
    "        self.vocab_size = 1000\n",
    "        self.embed_dim = 300  \n",
    "        self.hidden_dim = 128\n",
    "        self.max_length = 128\n",
    "        self.num_classes = None \n",
    "        self.batch_size = 16\n",
    "        self.learning_rate = 1e-3\n",
    "        self.weight_decay = 0.01\n",
    "        self.max_grad_norm = 1.0\n",
    "        self.max_epochs = 30\n",
    "        self.accumulation_steps = 4\n",
    "        self.label_smoothing = 0.1 \n",
    "\n",
    "        self.model_dir = \"models\"\n",
    "        self.model_name = \"best_unified_model.pth\"\n",
    "        self.model_path = os.path.join(self.model_dir, self.model_name)\n",
    "\n",
    "        self.rule_feature_dim = None\n",
    "        self.svm_feature_dim = 5000\n",
    "        self.projection_dim = 100\n",
    "\n",
    "        self.glove_file = \"glove.6B.300d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tacred_dataset():\n",
    "    \"\"\"Load TACRED dataset from the dataset directory.\"\"\"\n",
    "    data_dir = \"dataset\"\n",
    "    with open(os.path.join(data_dir, \"train.json\"), \"r\") as f:\n",
    "        train_data = json.load(f)\n",
    "    with open(os.path.join(data_dir, \"dev.json\"), \"r\") as f:\n",
    "        dev_data = json.load(f)\n",
    "    with open(os.path.join(data_dir, \"test.json\"), \"r\") as f:\n",
    "        test_data = json.load(f)\n",
    "    return train_data, dev_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(glove_file, tokenizer, embed_dim=300):\n",
    "    \"\"\"\n",
    "    Loads pretrained GloVe embeddings and returns a numpy array\n",
    "    aligned with the tokenizer's word indices.\n",
    "    \"\"\"\n",
    "    vocab_size = len(tokenizer.token2id)\n",
    "    embeddings = np.random.normal(scale=0.1, size=(vocab_size, embed_dim)).astype(np.float32)\n",
    "\n",
    "    glove_index = {}\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            split_line = line.strip().split()\n",
    "            word = split_line[0]\n",
    "            vec = np.array(split_line[1:], dtype=np.float32)\n",
    "            glove_index[word] = vec\n",
    "\n",
    "    for token, idx in tokenizer.token2id.items():\n",
    "        if token in glove_index:\n",
    "            embeddings[idx] = glove_index[token]\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    Simple tokenizer that builds a vocabulary from tokens.\n",
    "    (Used by the BiLSTM branch.)\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.token2id = {}\n",
    "        self.id2token = {}\n",
    "        self.special_tokens = {\n",
    "            '<PAD>': 0,\n",
    "            '<UNK>': 1,\n",
    "            '<CLS>': 2,\n",
    "            '<SEP>': 3\n",
    "        }\n",
    "\n",
    "    def build_vocab(self, texts):\n",
    "        # texts is a list of token lists\n",
    "        self.token2id = self.special_tokens.copy()\n",
    "        current_idx = len(self.special_tokens)\n",
    "        token_freq = Counter()\n",
    "        for text in texts:\n",
    "            token_freq.update(text)\n",
    "        for token, _ in token_freq.most_common(self.vocab_size - len(self.special_tokens)):\n",
    "            self.token2id[token] = current_idx\n",
    "            current_idx += 1\n",
    "        self.id2token = {v: k for k, v in self.token2id.items()}\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return [self.token2id.get(token, self.special_tokens['<UNK>']) for token in tokens]\n",
    "\n",
    "    def convert_ids_to_tokens(self, ids):\n",
    "        return [self.id2token.get(id_, '<UNK>') for id_ in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads=4):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "\n",
    "        self.query = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.key = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.value = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.output = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        Q = self.query(query).view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "        K = self.key(key).view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "        V = self.value(value).view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "\n",
    "        Q = Q.transpose(1, 2)\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attention_weights, V)\n",
    "\n",
    "        context = context.transpose(1, 2).contiguous().view(\n",
    "            batch_size, -1, self.num_heads * self.head_dim)\n",
    "        output = self.output(context)\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMWithAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(config.vocab_size, config.embed_dim, padding_idx=0)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.lstm = nn.LSTM(\n",
    "            config.embed_dim,\n",
    "            config.hidden_dim,\n",
    "            num_layers=2,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=0.3\n",
    "        )\n",
    "        self.attn_fc = nn.Linear(config.hidden_dim * 2, config.hidden_dim * 2)  # for simple attention\n",
    "        self.fc1 = nn.Linear(config.hidden_dim * 2, config.embed_dim)\n",
    "\n",
    "    def forward(self, token_ids, positions=None, entity_masks=None, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for BiLSTM with a simple self-attention mechanism.\n",
    "        \n",
    "        \"\"\"\n",
    "        x = self.embedding(token_ids)\n",
    "        x = self.dropout(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "\n",
    "        weights = self.attn_fc(lstm_out)  \n",
    "        weights = torch.tanh(weights)\n",
    "        weights = weights.sum(dim=-1)  \n",
    "\n",
    "  \n",
    "        if attention_mask is not None:\n",
    "            weights = weights.masked_fill(attention_mask == 0, -1e9)\n",
    "        attn_scores = F.softmax(weights, dim=1).unsqueeze(-1) \n",
    "\n",
    "       \n",
    "        weighted_out = lstm_out * attn_scores\n",
    "        rep = weighted_out.sum(dim=1)  \n",
    "\n",
    "        rep = F.relu(self.fc1(rep))  \n",
    "        rep = self.dropout(rep)\n",
    "        return rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuleBasedExtractor:\n",
    "    \"\"\"\n",
    "    Simple rule-based extractor using regex patterns.\n",
    "    Instead of returning a prediction, we will use it to create a one-hot\n",
    "    feature vector (over the set of possible relations).\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.patterns = {\n",
    "            'org:founded_by': [\n",
    "                r'.found.',\n",
    "                r'.start.',\n",
    "                r'.create.',\n",
    "                r'.establish.'\n",
    "            ],\n",
    "            'per:employee_of': [\n",
    "                r'.work.',\n",
    "                r'.join.',\n",
    "                r'.lead.',\n",
    "                r'.head.'\n",
    "            ],\n",
    "            'org:city_of_headquarters': [\n",
    "                r'.located.*in.',\n",
    "                r'.based.*in.',\n",
    "                r'.headquartered.*in.'\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    def predict(self, text, subj_span, obj_span):\n",
    "        try:\n",
    "            tokens = text.split()\n",
    "            # Use the text between subject and object (assuming subj then obj)\n",
    "            between_text = ' '.join(tokens[subj_span[1]:obj_span[0]])\n",
    "            for relation, patterns in self.patterns.items():\n",
    "                for pattern in patterns:\n",
    "                    if re.search(pattern, between_text, re.IGNORECASE):\n",
    "                        return relation\n",
    "            return \"no_relation\"\n",
    "        except Exception:\n",
    "            return \"no_relation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rule_features(text, subj_span, obj_span, relation_map, rule_extractor):\n",
    "    \"\"\"\n",
    "    Computes a one-hot vector (of dimension len(relation_map))\n",
    "    where the index corresponding to the rule-based prediction is 1.\n",
    "    \"\"\"\n",
    "    pred = rule_extractor.predict(text, subj_span, obj_span)\n",
    "    one_hot = np.zeros(len(relation_map), dtype=np.float32)\n",
    "    if pred in relation_map:\n",
    "        one_hot[relation_map[pred]] = 1.0\n",
    "    return one_hot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_svm_features(text, subj_span, obj_span, tfidf_vectorizer):\n",
    "    \"\"\"\n",
    "    Computes SVM-style TF-IDF features.\n",
    "    Uses the pattern: \"subj [SEP] between [SEP] obj\"\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "    subj = ' '.join(tokens[subj_span[0]:subj_span[1]])\n",
    "    obj = ' '.join(tokens[obj_span[0]:obj_span[1]])\n",
    "    between = ' '.join(tokens[subj_span[1]:obj_span[0]])\n",
    "    feature_str = f\"{subj} [SEP] {between} [SEP] {obj}\"\n",
    "    vec = tfidf_vectorizer.transform([feature_str]).toarray()[0]\n",
    "    return vec.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnifiedRelationDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, config, relation_map, tfidf_vectorizer, rule_extractor):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.config = config\n",
    "        self.relation_map = relation_map\n",
    "        self.tfidf_vectorizer = tfidf_vectorizer\n",
    "        self.rule_extractor = rule_extractor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.data[idx]\n",
    "        tokens = example['token']\n",
    "        subj_span = (example['subj_start'], example['subj_end'])\n",
    "        obj_span = (example['obj_start'], example['obj_end'])\n",
    "\n",
    "        # Tokenize\n",
    "        token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        if len(token_ids) < self.config.max_length:\n",
    "            token_ids += [0] * (self.config.max_length - len(token_ids))\n",
    "        else:\n",
    "            token_ids = token_ids[:self.config.max_length]\n",
    "\n",
    "        positions = torch.arange(self.config.max_length)\n",
    "        entity_masks = torch.zeros(self.config.max_length, dtype=torch.long)\n",
    "        attention_mask = torch.tensor([1 if tid != 0 else 0 for tid in token_ids])\n",
    "\n",
    "        for i in range(subj_span[0], min(subj_span[1], self.config.max_length)):\n",
    "            entity_masks[i] = 1\n",
    "        for i in range(obj_span[0], min(obj_span[1], self.config.max_length)):\n",
    "            entity_masks[i] = 2 if entity_masks[i] != 1 else 3\n",
    "\n",
    "        text = ' '.join(tokens)\n",
    "        rule_feat = compute_rule_features(text, subj_span, obj_span, self.relation_map, self.rule_extractor)\n",
    "        svm_feat = compute_svm_features(text, subj_span, obj_span, self.tfidf_vectorizer)\n",
    "\n",
    "        return {\n",
    "            'token_ids': torch.tensor(token_ids, dtype=torch.long),\n",
    "            'positions': positions,  # shape [max_length]\n",
    "            'entity_masks': entity_masks,\n",
    "            'attention_mask': attention_mask,\n",
    "            'label': torch.tensor(self.relation_map[example['relation']], dtype=torch.long),\n",
    "            'rule_features': torch.tensor(rule_feat, dtype=torch.float),\n",
    "            'svm_features': torch.tensor(svm_feat, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothingLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Replaces CrossEntropy with label smoothing, so the model doesn't become\n",
    "    too confident in a single class. This can help with class imbalance.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.smoothing = smoothing\n",
    "        self.kl_div = nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(logits)\n",
    "            true_dist.fill_(self.smoothing / (self.num_classes - 1))\n",
    "            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.smoothing)\n",
    "        log_probs = F.log_softmax(logits, dim=1)\n",
    "        return self.kl_div(log_probs, true_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnifiedRelationExtractor(nn.Module):\n",
    "    def __init__(self, config, tfidf_dim, rule_dim):\n",
    "        \"\"\"\n",
    "        tfidf_dim: dimension of the TF-IDF vector (e.g. 5000)\n",
    "        rule_dim: dimension of the rule-based one-hot vector (equal to num_relations)\n",
    "        \"\"\"\n",
    "        super(UnifiedRelationExtractor, self).__init__()\n",
    "        self.encoder = BiLSTMWithAttention(config)\n",
    "\n",
    "        # Project rule features and svm features to a common dimension\n",
    "        self.rule_fc = nn.Linear(rule_dim, config.projection_dim)\n",
    "        self.svm_fc = nn.Linear(tfidf_dim, config.projection_dim)\n",
    "\n",
    "       \n",
    "        in_dim = config.embed_dim + 2 * config.projection_dim  # 300 + 2*100 = 500\n",
    "\n",
    "        hidden = 128\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden, config.num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, token_ids, positions, entity_masks, attention_mask, rule_features, svm_features):\n",
    "        lstm_rep = self.encoder(\n",
    "            token_ids,\n",
    "            positions=positions,\n",
    "            entity_masks=entity_masks,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        rule_rep = F.relu(self.rule_fc(rule_features))  # shape: [batch_size, 100]\n",
    "        svm_rep = F.relu(self.svm_fc(svm_features))     # shape: [batch_size, 100]\n",
    "\n",
    "        combined = torch.cat([lstm_rep, rule_rep, svm_rep], dim=1)\n",
    "\n",
    "        logits = self.classifier(combined)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_unified(model, train_loader, dev_loader, config, device):\n",
    "    if config.label_smoothing > 0:\n",
    "        criterion = LabelSmoothingLoss(config.num_classes, smoothing=config.label_smoothing)\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=2, factor=0.5)\n",
    "\n",
    "    for epoch in range(config.max_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{config.max_epochs}')\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            token_ids = batch['token_ids'].to(device)\n",
    "            positions = batch['positions'].to(device)        \n",
    "            entity_masks = batch['entity_masks'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            rule_features = batch['rule_features'].to(device)\n",
    "            svm_features = batch['svm_features'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            logits = model(\n",
    "                token_ids, positions, entity_masks, attention_mask,\n",
    "                rule_features, svm_features\n",
    "            )\n",
    "            loss = criterion(logits, labels)\n",
    "            loss = loss / config.accumulation_steps\n",
    "            loss.backward()\n",
    "\n",
    "            if (batch_idx + 1) % config.accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item() * config.accumulation_steps\n",
    "            progress_bar.set_postfix({'loss': total_loss / (batch_idx + 1)})\n",
    "\n",
    "        dev_metrics = evaluate_unified(model, dev_loader, device)\n",
    "        logger.info(\n",
    "            f\"Epoch {epoch+1}/{config.max_epochs} - \"\n",
    "            f\"Train Loss: {total_loss/len(train_loader):.4f} - \"\n",
    "            f\"Dev F1: {dev_metrics['f1']:.4f}\"\n",
    "        )\n",
    "\n",
    "        scheduler.step(dev_metrics['f1'])\n",
    "\n",
    "    logger.info(\"Training completed. Saving final model...\")\n",
    "    torch.save(model.state_dict(), config.model_path)\n",
    "    logger.info(f\"Final model saved to {config.model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_unified(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            token_ids = batch['token_ids'].to(device)\n",
    "            positions = batch['positions'].to(device)\n",
    "            entity_masks = batch['entity_masks'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            rule_features = batch['rule_features'].to(device)\n",
    "            svm_features = batch['svm_features'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            logits = model(\n",
    "                token_ids, positions, entity_masks, attention_mask,\n",
    "                rule_features, svm_features\n",
    "            )\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return {\n",
    "        'f1': f1_score(all_labels, all_preds, average='macro'),\n",
    "        'accuracy': accuracy_score(all_labels, all_preds)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-07 12:28:49,129 - INFO - Loading TACRED dataset...\n",
      "2025-03-07 12:29:38,541 - INFO - Test Accuracy: 0.7592, Test F1: 0.1010\n",
      "2025-03-07 12:29:38,542 - INFO - Sample predictions on test instances:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: For Cephalon , too , the payoff was a bargain : Chief executive Frank Baldino Jr. acknowledged that it made about $ 4 billion `` that no one expected . ''\n",
      "Subject: Frank Baldino\n",
      "Object: \n",
      "Predicted Relation: no_relation\n",
      "\n",
      "Input: -LCB- EXCERPT -RCB- New York Times , United States Jamie Leigh Jones , left , a former employee for the military contractor KBR , told Congress that she had been gang-raped by co-workers in Iraq in 2005 ... .\n",
      "Subject: Jamie Leigh\n",
      "Object: \n",
      "Predicted Relation: no_relation\n",
      "\n",
      "Input: Information Services Group purchased TPI in October .\n",
      "Subject: \n",
      "Object: \n",
      "Predicted Relation: no_relation\n",
      "\n",
      "Input: Midfielders : Yang Hao , Yan Xiangchuang , Deng Zhuoxiang , Zhou Haibin , Yu Tao , Feng Renliang , Chen Tao , Yu Hai , Zhao Xuri , Yu Hanchao ,\n",
      "Subject: Chen\n",
      "Object: Zhou\n",
      "Predicted Relation: no_relation\n",
      "\n",
      "Input: Letter from Havana : There is only one kosher butcher in Cuba , but if community president Adela Dworin knows her community -- and she does -- there is also only one Jew in the country who buys kosher meat .\n",
      "Subject: \n",
      "Object: \n",
      "Predicted Relation: no_relation\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "\n",
    "    config = Config()\n",
    "    os.makedirs(config.model_dir, exist_ok=True)\n",
    "\n",
    "    # Load TACRED dataset\n",
    "    logger.info(\"Loading TACRED dataset...\")\n",
    "    train_data, dev_data, test_data = load_tacred_dataset()\n",
    "\n",
    "    all_data = train_data + dev_data + test_data\n",
    "    unique_relations = sorted(set(ex['relation'] for ex in all_data))\n",
    "    relation_map = {rel: idx for idx, rel in enumerate(unique_relations)}\n",
    "    config.num_classes = len(relation_map)\n",
    "    config.rule_feature_dim = config.num_classes  # one-hot over relations\n",
    "\n",
    "    all_tokens = [token for ex in train_data for token in ex['token']]\n",
    "    tokenizer = Tokenizer(config.vocab_size)\n",
    "    tokenizer.build_vocab([all_tokens])\n",
    "\n",
    "    # Load GloVe embeddings\n",
    "    glove_matrix = load_glove_embeddings(config.glove_file, tokenizer, embed_dim=config.embed_dim)\n",
    "\n",
    "    \n",
    "    train_texts = []\n",
    "    for ex in train_data:\n",
    "        tokens = ex['token']\n",
    "        subj_span = (ex['subj_start'], ex['subj_end'])\n",
    "        obj_span = (ex['obj_start'], ex['obj_end'])\n",
    "        subj = ' '.join(tokens[subj_span[0]:subj_span[1]])\n",
    "        obj = ' '.join(tokens[obj_span[0]:obj_span[1]])\n",
    "        between = ' '.join(tokens[subj_span[1]:obj_span[0]])\n",
    "        train_texts.append(f\"{subj} [SEP] {between} [SEP] {obj}\")\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=config.svm_feature_dim)\n",
    "    tfidf_vectorizer.fit(train_texts)\n",
    "    actual_feature_dim = len(tfidf_vectorizer.get_feature_names_out())\n",
    "    config.svm_feature_dim = actual_feature_dim\n",
    "\n",
    "\n",
    "    rule_extractor = RuleBasedExtractor()\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = UnifiedRelationDataset(train_data, tokenizer, config, relation_map,\n",
    "                                           tfidf_vectorizer, rule_extractor)\n",
    "    dev_dataset = UnifiedRelationDataset(dev_data, tokenizer, config, relation_map,\n",
    "                                         tfidf_vectorizer, rule_extractor)\n",
    "    test_dataset = UnifiedRelationDataset(test_data, tokenizer, config, relation_map,\n",
    "                                          tfidf_vectorizer, rule_extractor)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "    dev_loader = DataLoader(dev_dataset, batch_size=config.batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config.batch_size)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = UnifiedRelationExtractor(\n",
    "        config,\n",
    "        tfidf_dim=config.svm_feature_dim,\n",
    "        rule_dim=config.rule_feature_dim\n",
    "    )\n",
    "    \n",
    "    model.encoder.embedding.weight.data.copy_(torch.from_numpy(glove_matrix))\n",
    "    model.to(device)\n",
    "\n",
    "    # ---------------------------\n",
    "    # Training Phase\n",
    "    # ---------------------------\n",
    "    # The training phase is commented out so that the professors can run inference\n",
    "    # on pre-trained model without needing to retrain it.\n",
    "    # logger.info(\"Starting training of the unified model...\")\n",
    "    # train_unified(model, train_loader, dev_loader, config, device)\n",
    "    \n",
    "    model.load_state_dict(torch.load(config.model_path, map_location=device))\n",
    "    test_metrics = evaluate_unified(model, test_loader, device)\n",
    "    logger.info(f\"Test Accuracy: {test_metrics['accuracy']:.4f}, Test F1: {test_metrics['f1']:.4f}\")\n",
    "\n",
    "    \n",
    "    logger.info(\"Sample predictions on test instances:\")\n",
    "    model.eval()\n",
    "    for ex in test_data[800:805]:\n",
    "        tokens = ex['token']\n",
    "        text = ' '.join(tokens)\n",
    "        subj_span = (ex['subj_start'], ex['subj_end'])\n",
    "        obj_span = (ex['obj_start'], ex['obj_end'])\n",
    "\n",
    "        # Prepare inputs for the BiLSTM branch\n",
    "        token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        if len(token_ids) < config.max_length:\n",
    "            token_ids += [0] * (config.max_length - len(token_ids))\n",
    "        else:\n",
    "            token_ids = token_ids[:config.max_length]\n",
    "\n",
    "        positions = torch.arange(config.max_length).unsqueeze(0).to(device)\n",
    "        entity_masks = torch.zeros(config.max_length, dtype=torch.long)\n",
    "        for i in range(subj_span[0], min(subj_span[1], config.max_length)):\n",
    "            entity_masks[i] = 1\n",
    "        for i in range(obj_span[0], min(obj_span[1], config.max_length)):\n",
    "            entity_masks[i] = 2 if entity_masks[i] != 1 else 3\n",
    "        entity_masks = entity_masks.unsqueeze(0).to(device)\n",
    "\n",
    "        attention_mask = torch.tensor([1 if tid != 0 else 0 for tid in token_ids]).unsqueeze(0).to(device)\n",
    "\n",
    "        \n",
    "        rule_feat = compute_rule_features(text, subj_span, obj_span, relation_map, rule_extractor)\n",
    "        svm_feat = compute_svm_features(text, subj_span, obj_span, tfidf_vectorizer)\n",
    "        rule_feat = torch.tensor(rule_feat, dtype=torch.float).unsqueeze(0).to(device)\n",
    "        svm_feat = torch.tensor(svm_feat, dtype=torch.float).unsqueeze(0).to(device)\n",
    "\n",
    "        token_ids_tensor = torch.tensor([token_ids], dtype=torch.long).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(\n",
    "                token_ids_tensor,\n",
    "                positions,\n",
    "                entity_masks,\n",
    "                attention_mask,\n",
    "                rule_feat,\n",
    "                svm_feat\n",
    "            )\n",
    "            pred_idx = torch.argmax(logits, dim=1).item()\n",
    "            pred_relation = list(relation_map.keys())[list(relation_map.values()).index(pred_idx)]\n",
    "\n",
    "        print(f\"\\nInput: {text}\")\n",
    "        print(f\"Subject: {' '.join(tokens[subj_span[0]:subj_span[1]])}\")\n",
    "        print(f\"Object: {' '.join(tokens[obj_span[0]:obj_span[1]])}\")\n",
    "        print(f\"Predicted Relation: {pred_relation}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
