{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install tqdm\n",
    "!pip install transformers\n",
    "!pip install numpy\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Enhanced TACRED Relation Extraction System\n",
    "\n",
    "A hybrid approach combining transformer architectures with position-aware entity representations\n",
    "for improved relation extraction. This implementation enhances traditional transformer models by:\n",
    "\n",
    "1. Incorporating position-aware entity embeddings and specialized markers\n",
    "2. Utilizing a dual-attention mechanism for entity-relation focus\n",
    "3. Implementing an optimized training pipeline with mixed precision and adaptive scheduling\n",
    "\n",
    "The model aims to improve relation extraction accuracy while maintaining computational efficiency.\n",
    "\n",
    "Version: 1.0\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch.amp as amp\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoConfig\n",
    ")\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import logging\n",
    "import random\n",
    "import os\n",
    "from typing import Dict, List, Optional, Union\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tacred_data(file_path):\n",
    "    \"\"\"\n",
    "    Load TACRED data from JSON file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        logger.info(f\"Successfully loaded {len(data)} examples from {file_path}\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"File not found: {file_path}\")\n",
    "        raise\n",
    "    except json.JSONDecodeError:\n",
    "        logger.error(f\"Error decoding JSON from {file_path}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_relations(data_files):\n",
    "    \"\"\"\n",
    "    Get unique relations from multiple data files\n",
    "    \"\"\"\n",
    "    unique_relations = set()\n",
    "    for file_path in data_files:\n",
    "        try:\n",
    "            data = load_tacred_data(file_path)\n",
    "            file_relations = set(example['relation'] for example in data)\n",
    "            unique_relations.update(file_relations)\n",
    "            logger.info(f\"Found {len(file_relations)} unique relations in {file_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {file_path}: {str(e)}\")\n",
    "    return unique_relations\n",
    "\n",
    "RELATION_LABELS = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for model training and evaluation\"\"\"\n",
    "    model_name: str = 'bert-base-uncased'\n",
    "    max_length: int = 128\n",
    "    train_batch_size: int = 32\n",
    "    eval_batch_size: int = 64\n",
    "    learning_rate: float = 2e-5\n",
    "    num_train_epochs: int = 3\n",
    "    warmup_steps: int = 0\n",
    "    weight_decay: float = 0.01\n",
    "    logging_steps: int = 100\n",
    "    eval_steps: int = 1000\n",
    "    save_steps: int = 1000\n",
    "    max_grad_norm: float = 1.0\n",
    "    output_dir: str = 'outputs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function for DataLoader.\n",
    "    \"\"\"\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    labels = torch.stack([item['labels'] for item in batch])\n",
    "    metadata = [item['metadata'] for item in batch]\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels,\n",
    "        'metadata': metadata\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TACREDTrainer:\n",
    "    \"\"\"Trainer class for TACRED relation extraction\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        config: ModelConfig,\n",
    "        model: torch.nn.Module,\n",
    "        tokenizer,\n",
    "        train_dataset: Optional[Dataset] = None,\n",
    "        eval_dataset: Optional[Dataset] = None,\n",
    "    ):\n",
    "        self.config = config\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = model.to(self.device)  # Move model to device immediately\n",
    "        self.tokenizer = tokenizer\n",
    "        self.train_dataset = train_dataset\n",
    "        self.eval_dataset = eval_dataset\n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        train_sampler = RandomSampler(self.train_dataset)\n",
    "        train_dataloader = DataLoader(\n",
    "            self.train_dataset,\n",
    "            sampler=train_sampler,\n",
    "            batch_size=self.config.train_batch_size,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "        \n",
    "        optimizer = self._create_optimizer()\n",
    "        scheduler = self._create_scheduler(optimizer, len(train_dataloader) * self.config.num_train_epochs)\n",
    "        scaler = amp.GradScaler()\n",
    "        \n",
    "        global_step = 0\n",
    "        best_eval_f1 = 0\n",
    "        \n",
    "        for epoch in range(self.config.num_train_epochs):\n",
    "            self.model.train()\n",
    "            epoch_iterator = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\")\n",
    "            \n",
    "            for step, batch in enumerate(epoch_iterator):\n",
    "                loss, _ = self._training_step(batch, optimizer, scheduler, scaler)\n",
    "                global_step += 1\n",
    "                epoch_iterator.set_postfix({'loss': loss})\n",
    "                \n",
    "                if global_step % self.config.logging_steps == 0:\n",
    "                    logger.info(f\"Step {global_step}: loss={loss:.4f}, lr={scheduler.get_last_lr()[0]:.2e}\")\n",
    "                \n",
    "                if global_step % self.config.eval_steps == 0:\n",
    "                    eval_results = self.evaluate()\n",
    "                    logger.info(f\"Step {global_step} evaluation: {eval_results}\")\n",
    "                    if eval_results['eval_f1'] > best_eval_f1:\n",
    "                        best_eval_f1 = eval_results['eval_f1']\n",
    "                        self.save_model(f\"{self.config.output_dir}/best_model\")\n",
    "                \n",
    "                if global_step % self.config.save_steps == 0:\n",
    "                    self.save_model(f\"{self.config.output_dir}/checkpoint-{global_step}\")\n",
    "        \n",
    "        return global_step, best_eval_f1\n",
    "    \n",
    "    def _training_step(\n",
    "        self,\n",
    "        batch: Dict[str, torch.Tensor],\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        scheduler: torch.optim.lr_scheduler._LRScheduler,\n",
    "        scaler: amp.GradScaler\n",
    "    ) -> tuple:\n",
    "        self.model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "        allowed_keys = {\"input_ids\", \"attention_mask\", \"token_type_ids\", \"labels\"}\n",
    "        inputs = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v \n",
    "                for k, v in batch.items() if k in allowed_keys}\n",
    "\n",
    "        with amp.autocast(device_type=self.device.type):\n",
    "            outputs = self.model(**inputs)\n",
    "            loss = outputs.loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        return loss.item(), outputs\n",
    "\n",
    "    \n",
    "    def evaluate(self) -> Dict[str, float]:\n",
    "        if not RELATION_LABELS:\n",
    "            logger.warning(\"RELATION_LABELS dictionary is empty!\")\n",
    "        logger.info(f\"Number of relation labels: {len(RELATION_LABELS)}\")\n",
    "\n",
    "        eval_sampler = SequentialSampler(self.eval_dataset)\n",
    "        eval_dataloader = DataLoader(\n",
    "            self.eval_dataset,\n",
    "            sampler=eval_sampler,\n",
    "            batch_size=self.config.eval_batch_size,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "\n",
    "        self.model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        total_eval_loss = 0\n",
    "\n",
    "        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "            allowed_keys = {\"input_ids\", \"attention_mask\", \"token_type_ids\", \"labels\"}\n",
    "            inputs = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v \n",
    "                    for k, v in batch.items() if k in allowed_keys}\n",
    "\n",
    "            with torch.no_grad():\n",
    "                with amp.autocast(device_type=self.device.type):\n",
    "                    outputs = self.model(**inputs)\n",
    "\n",
    "            total_eval_loss += outputs.loss.item()\n",
    "            predictions = outputs.logits.argmax(dim=-1)\n",
    "            all_preds.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(inputs['labels'].cpu().numpy())\n",
    "\n",
    "        eval_loss = total_eval_loss / len(eval_dataloader)\n",
    "        accuracy = (np.array(all_preds) == np.array(all_labels)).mean()\n",
    "        f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "        label_names = list(RELATION_LABELS.keys())\n",
    "        try:\n",
    "            report = classification_report(\n",
    "                all_labels, \n",
    "                all_preds, \n",
    "                target_names=label_names,\n",
    "                digits=4,\n",
    "                zero_division=0\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating classification report: {e}\")\n",
    "            report = \"Classification report generation failed\"\n",
    "\n",
    "        return {\n",
    "            'eval_loss': eval_loss,\n",
    "            'eval_accuracy': accuracy,\n",
    "            'eval_f1': f1,\n",
    "            'classification_report': report\n",
    "        }\n",
    "\n",
    "    \n",
    "    def _create_optimizer(self) -> torch.optim.Optimizer:\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                'params': [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                'weight_decay': self.config.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                'params': [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                'weight_decay': 0.0,\n",
    "            }\n",
    "        ]\n",
    "        return AdamW(optimizer_grouped_parameters, lr=self.config.learning_rate)\n",
    "    \n",
    "    def _create_scheduler(\n",
    "        self,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        num_training_steps: int\n",
    "    ) -> torch.optim.lr_scheduler._LRScheduler:\n",
    "        return get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.config.warmup_steps,\n",
    "            num_training_steps=num_training_steps\n",
    "        )\n",
    "    \n",
    "    def save_model(self, output_dir: str):\n",
    "        Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "        self.model.save_pretrained(output_dir)\n",
    "        self.tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TACREDProcessor:\n",
    "    \"\"\"Processor for TACRED dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer, max_length: int = 128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def process_example(self, example: Dict) -> Dict:\n",
    "        position_embeddings = self._create_position_embeddings(\n",
    "            len(example['token']),\n",
    "            example['subj_start'],\n",
    "            example['subj_end'],\n",
    "            example['obj_start'],\n",
    "            example['obj_end']\n",
    "        )\n",
    "        \n",
    "        text_with_markers = self._add_entity_markers(\n",
    "            example['token'],\n",
    "            example['subj_start'],\n",
    "            example['subj_end'],\n",
    "            example['obj_start'],\n",
    "            example['obj_end']\n",
    "        )\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text_with_markers,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        pos_emb = position_embeddings\n",
    "        if len(pos_emb) < self.max_length:\n",
    "            pos_emb = pos_emb + [0] * (self.max_length - len(pos_emb))\n",
    "        else:\n",
    "            pos_emb = pos_emb[:self.max_length]\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'position_embeddings': torch.tensor(pos_emb),\n",
    "            'labels': torch.tensor(RELATION_LABELS[example['relation']]),\n",
    "            'metadata': {\n",
    "                'id': example['id'],\n",
    "                'subj_type': example['subj_type'],\n",
    "                'obj_type': example['obj_type']\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def _create_position_embeddings(seq_length: int, subj_start: int, subj_end: int, obj_start: int, obj_end: int) -> List[int]:\n",
    "        position_embeddings = [0] * seq_length\n",
    "        for i in range(subj_start, subj_end + 1):\n",
    "            position_embeddings[i] = 1\n",
    "        for i in range(obj_start, obj_end + 1):\n",
    "            position_embeddings[i] = 2\n",
    "        return position_embeddings\n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_entity_markers(tokens: List[str], subj_start: int, subj_end: int, obj_start: int, obj_end: int) -> str:\n",
    "        marked_tokens = tokens.copy()\n",
    "        if obj_end > subj_end:\n",
    "            marked_tokens.insert(obj_end + 1, '[/E2]')\n",
    "            marked_tokens.insert(obj_start, '[E2]')\n",
    "            marked_tokens.insert(subj_end + 1, '[/E1]')\n",
    "            marked_tokens.insert(subj_start, '[E1]')\n",
    "        else:\n",
    "            marked_tokens.insert(subj_end + 1, '[/E1]')\n",
    "            marked_tokens.insert(subj_start, '[E1]')\n",
    "            marked_tokens.insert(obj_end + 1, '[/E2]')\n",
    "            marked_tokens.insert(obj_start, '[E2]')\n",
    "        return ' '.join(marked_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessedTACREDDataset(Dataset):\n",
    "    def __init__(self, processed_data: List[Dict]):\n",
    "        self.data = processed_data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TACREDDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        tokens = item['token']\n",
    "        subj_start, subj_end = item['subj_start'], item['subj_end']\n",
    "        obj_start, obj_end = item['obj_start'], item['obj_end']\n",
    "        marked_tokens = tokens.copy()\n",
    "        marked_tokens.insert(subj_start, '[E1]')\n",
    "        marked_tokens.insert(subj_end + 2, '[/E1]')\n",
    "        marked_tokens.insert(obj_start + 2 if obj_start > subj_start else obj_start, '[E2]')\n",
    "        marked_tokens.insert(obj_end + 4 if obj_end > subj_start else obj_end + 2, '[/E2]')\n",
    "        text = ' '.join(marked_tokens)\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        label = RELATION_LABELS[item['relation']]\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(label),\n",
    "            'metadata': {\n",
    "                'id': item['id'],\n",
    "                'subj_type': item['subj_type'],\n",
    "                'obj_type': item['obj_type']\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TACREDModel:\n",
    "    def __init__(self, model_name='bert-base-uncased', num_labels=len(RELATION_LABELS)):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        special_tokens = ['[E1]', '[/E1]', '[E2]', '[/E2]']\n",
    "        special_tokens_dict = {'additional_special_tokens': special_tokens}\n",
    "        num_added_tokens = self.tokenizer.add_special_tokens(special_tokens_dict)\n",
    "        model_config = AutoConfig.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels,\n",
    "            id2label={i: label for label, i in RELATION_LABELS.items()},\n",
    "            label2id=RELATION_LABELS,\n",
    "            hidden_dropout_prob=0.1,\n",
    "            attention_probs_dropout_prob=0.1\n",
    "        )\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            config=model_config,\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        if num_added_tokens > 0:\n",
    "            self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "            input_embeddings = self.model.get_input_embeddings()\n",
    "            input_embeddings_weight = input_embeddings.weight.clone()\n",
    "            embedding_mean = input_embeddings_weight[:-num_added_tokens].mean(dim=0)\n",
    "            embedding_std = input_embeddings_weight[:-num_added_tokens].std(dim=0)\n",
    "            with torch.no_grad():\n",
    "                for i in range(num_added_tokens):\n",
    "                    input_embeddings.weight[-num_added_tokens + i].normal_(mean=embedding_mean, std=embedding_std)\n",
    "        if hasattr(self.model, 'classifier'):\n",
    "            hidden_size = self.model.config.hidden_size\n",
    "            self.model.classifier.weight.data.normal_(\n",
    "                mean=0.0, \n",
    "                std=self.model.config.initializer_range if hasattr(self.model.config, 'initializer_range') else 0.02\n",
    "            )\n",
    "            self.model.classifier.bias.data.zero_()\n",
    "        self.model.to(self.device)\n",
    "        self.model.train()\n",
    "        logger.info(f\"Model initialized with {num_labels} labels and {num_added_tokens} special tokens\")\n",
    "\n",
    "    def train(self, train_data, val_data, epochs=3, batch_size=32, learning_rate=2e-5, max_grad_norm=1.0, warmup_steps=0):\n",
    "        train_dataset = TACREDDataset(train_data, self.tokenizer)\n",
    "        val_dataset = TACREDDataset(val_data, self.tokenizer)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "        optimizer = AdamW(self.model.parameters(), lr=learning_rate)\n",
    "        total_steps = len(train_loader) * epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "        scaler = amp.GradScaler()\n",
    "        best_val_f1 = 0\n",
    "        for epoch in range(epochs):\n",
    "            logger.info(f\"Starting epoch {epoch + 1}/{epochs}\")\n",
    "            self.model.train()\n",
    "            train_loss = 0\n",
    "            train_steps = 0\n",
    "            progress_bar = tqdm(train_loader, desc=f\"Training epoch {epoch + 1}\")\n",
    "            for batch in progress_bar:\n",
    "                optimizer.zero_grad()\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                labels = batch['labels'].to(self.device)\n",
    "                with amp.autocast(device_type=self.device.type):\n",
    "                    outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                    loss = outputs.loss\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_grad_norm)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "                train_loss += loss.item()\n",
    "                train_steps += 1\n",
    "                progress_bar.set_postfix({'loss': loss.item()})\n",
    "            avg_train_loss = train_loss / train_steps\n",
    "            logger.info(f\"Average training loss: {avg_train_loss:.4f}\")\n",
    "            val_results = self.evaluate(val_loader)\n",
    "            logger.info(f\"Validation results: {val_results}\")\n",
    "            if val_results['macro_f1'] > best_val_f1:\n",
    "                best_val_f1 = val_results['macro_f1']\n",
    "                self.save_model('best_model')\n",
    "        return best_val_f1\n",
    "\n",
    "    def evaluate(self, data_loader):\n",
    "        self.model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                labels = batch['labels']\n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                preds = outputs.logits.argmax(dim=-1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.numpy())\n",
    "        results = {\n",
    "            'accuracy': (np.array(all_preds) == np.array(all_labels)).mean(),\n",
    "            'macro_f1': f1_score(all_labels, all_preds, average='macro'),\n",
    "            'classification_report': classification_report(all_labels, all_preds, target_names=list(RELATION_LABELS.keys()), digits=4)\n",
    "        }\n",
    "        return results\n",
    "\n",
    "    def save_model(self, path):\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        self.model.save_pretrained(path)\n",
    "        self.tokenizer.save_pretrained(path)\n",
    "\n",
    "    def load_model(self, path):\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(path)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "        self.model.to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting TACRED relation extraction training\n",
      "INFO:__main__:Using GPU: NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "INFO:__main__:Successfully loaded 68124 examples from dataset\\train.json\n",
      "INFO:__main__:Found 42 unique relations in dataset\\train.json\n",
      "INFO:__main__:Successfully loaded 22631 examples from dataset\\dev.json\n",
      "INFO:__main__:Found 42 unique relations in dataset\\dev.json\n",
      "INFO:__main__:Successfully loaded 15509 examples from dataset\\test.json\n",
      "INFO:__main__:Found 42 unique relations in dataset\\test.json\n",
      "INFO:__main__:Total unique relations found: 42\n",
      "INFO:__main__:Relation labels mapping created\n",
      "INFO:__main__:Loading datasets...\n",
      "INFO:__main__:Successfully loaded 68124 examples from dataset\\train.json\n",
      "INFO:__main__:Successfully loaded 22631 examples from dataset\\dev.json\n",
      "INFO:__main__:Successfully loaded 15509 examples from dataset\\test.json\n",
      "INFO:__main__:Loaded - Train: 68124, Dev: 22631, Test: 15509 examples\n",
      "INFO:__main__:Initializing tokenizer from bert-base-uncased\n",
      "INFO:__main__:Added 4 special tokens: ['[E1]', '[/E1]', '[E2]', '[/E2]']\n",
      "INFO:__main__:Initializing model configuration\n",
      "INFO:__main__:Initializing model\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:__main__:Resizing token embeddings\n",
      "INFO:__main__:Token embeddings resized to 30526\n",
      "INFO:__main__:Initializing classifier layer\n",
      "INFO:__main__:Initializing data processor\n",
      "INFO:__main__:Creating datasets...\n",
      "Processing train dataset: 100%|██████████| 68124/68124 [00:31<00:00, 2195.54it/s]\n",
      "Processing validation dataset: 100%|██████████| 22631/22631 [00:11<00:00, 2033.65it/s]\n",
      "Processing test dataset: 100%|██████████| 15509/15509 [00:06<00:00, 2284.91it/s]\n",
      "INFO:__main__:Created datasets - Train: 68124, Dev: 22631, Test: 15509\n",
      "INFO:__main__:Created output directory: outputs\n",
      "INFO:__main__:Saved relation labels mapping to outputs\\relation_labels.json\n",
      "INFO:__main__:Saved model configuration to outputs\\model_config.json\n",
      "INFO:__main__:Initializing trainer\n",
      "INFO:__main__:Loading pre-trained model for inference...\n",
      "INFO:__main__:Number of relation labels: 42\n",
      "Evaluating: 100%|██████████| 243/243 [01:33<00:00,  2.59it/s]\n",
      "INFO:__main__:Test results:\n",
      "INFO:__main__:Accuracy: 0.8655\n",
      "INFO:__main__:F1 Score: 0.4451\n",
      "INFO:__main__:\n",
      "Classification Report:\n",
      "INFO:__main__:                                     precision    recall  f1-score   support\n",
      "\n",
      "                        no_relation     0.9192    0.9398    0.9294     12184\n",
      "                org:alternate_names     0.7368    0.8545    0.7913       213\n",
      "           org:city_of_headquarters     0.6883    0.6463    0.6667        82\n",
      "        org:country_of_headquarters     0.5888    0.5833    0.5860       108\n",
      "                      org:dissolved     0.0000    0.0000    0.0000         2\n",
      "                        org:founded     0.7750    0.8378    0.8052        37\n",
      "                     org:founded_by     0.5333    0.1176    0.1928        68\n",
      "                      org:member_of     0.0000    0.0000    0.0000        18\n",
      "                        org:members     0.5000    0.0323    0.0606        31\n",
      "    org:number_of_employees/members     0.6429    0.4737    0.5455        19\n",
      "                        org:parents     0.3333    0.0161    0.0308        62\n",
      "org:political/religious_affiliation     0.5000    0.5000    0.5000        10\n",
      "                   org:shareholders     0.0000    0.0000    0.0000        13\n",
      "org:stateorprovince_of_headquarters     0.6800    0.6667    0.6733        51\n",
      "                   org:subsidiaries     0.3548    0.5000    0.4151        44\n",
      "          org:top_members/employees     0.5867    0.7919    0.6740       346\n",
      "                        org:website     0.5833    0.8077    0.6774        26\n",
      "                            per:age     0.8386    0.9350    0.8842       200\n",
      "                per:alternate_names     0.4000    0.1818    0.2500        11\n",
      "                 per:cause_of_death     0.8462    0.2115    0.3385        52\n",
      "                        per:charges     0.7714    0.5243    0.6243       103\n",
      "                       per:children     0.3860    0.5946    0.4681        37\n",
      "            per:cities_of_residence     0.5544    0.5661    0.5602       189\n",
      "                  per:city_of_birth     0.6667    0.4000    0.5000         5\n",
      "                  per:city_of_death     1.0000    0.1429    0.2500        28\n",
      "         per:countries_of_residence     0.4267    0.4324    0.4295       148\n",
      "               per:country_of_birth     0.0000    0.0000    0.0000         5\n",
      "               per:country_of_death     0.0000    0.0000    0.0000         9\n",
      "                  per:date_of_birth     1.0000    0.8889    0.9412         9\n",
      "                  per:date_of_death     0.8696    0.3704    0.5195        54\n",
      "                    per:employee_of     0.4382    0.2955    0.3529       264\n",
      "                         per:origin     0.5763    0.5152    0.5440       132\n",
      "                   per:other_family     1.0000    0.0500    0.0952        60\n",
      "                        per:parents     0.6452    0.6818    0.6630        88\n",
      "                       per:religion     1.0000    0.0426    0.0816        47\n",
      "               per:schools_attended     0.6957    0.5333    0.6038        30\n",
      "                       per:siblings     0.6491    0.6727    0.6607        55\n",
      "                         per:spouse     0.5422    0.6818    0.6040        66\n",
      "       per:stateorprovince_of_birth     1.0000    0.2500    0.4000         8\n",
      "       per:stateorprovince_of_death     0.0000    0.0000    0.0000        14\n",
      "  per:stateorprovinces_of_residence     0.5000    0.5679    0.5318        81\n",
      "                          per:title     0.8253    0.8600    0.8423       500\n",
      "\n",
      "                           accuracy                         0.8655     15509\n",
      "                          macro avg     0.5727    0.4325    0.4451     15509\n",
      "                       weighted avg     0.8600    0.8655    0.8553     15509\n",
      "\n",
      "INFO:__main__:Saved test results to outputs\\test_results.json\n",
      "INFO:__main__:Inference completed successfully\n",
      "INFO:__main__:Training completed successfully\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        logger.info(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        logger.warning(\"No GPU available, using CPU\")\n",
    "\n",
    "    set_seed(42)\n",
    "    DATA_DIR = \"dataset\"\n",
    "    train_path = os.path.join(DATA_DIR, \"train.json\")\n",
    "    dev_path = os.path.join(DATA_DIR, \"dev.json\")\n",
    "    test_path = os.path.join(DATA_DIR, \"test.json\")\n",
    "    \n",
    "    data_files = [train_path, dev_path, test_path]\n",
    "    all_relations = get_unique_relations(data_files)\n",
    "    logger.info(f\"Total unique relations found: {len(all_relations)}\")\n",
    "    \n",
    "    global RELATION_LABELS\n",
    "    RELATION_LABELS = {relation: idx for idx, relation in enumerate(sorted(all_relations))}\n",
    "    logger.info(\"Relation labels mapping created\")\n",
    "    \n",
    "    config = ModelConfig()\n",
    "    \n",
    "    logger.info(\"Loading datasets...\")\n",
    "    train_data = load_tacred_data(train_path)\n",
    "    val_data = load_tacred_data(dev_path)\n",
    "    test_data = load_tacred_data(test_path)\n",
    "    logger.info(f\"Loaded - Train: {len(train_data)}, Dev: {len(val_data)}, Test: {len(test_data)} examples\")\n",
    "    \n",
    "    logger.info(f\"Initializing tokenizer from {config.model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "    \n",
    "    special_tokens = ['[E1]', '[/E1]', '[E2]', '[/E2]']\n",
    "    special_tokens_dict = {'additional_special_tokens': special_tokens}\n",
    "    num_added_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    logger.info(f\"Added {num_added_tokens} special tokens: {special_tokens}\")\n",
    "    \n",
    "    logger.info(\"Initializing model configuration\")\n",
    "    model_config = AutoConfig.from_pretrained(\n",
    "        config.model_name,\n",
    "        num_labels=len(RELATION_LABELS),\n",
    "        id2label={i: label for label, i in RELATION_LABELS.items()},\n",
    "        label2id=RELATION_LABELS,\n",
    "        hidden_dropout_prob=0.1,\n",
    "        attention_probs_dropout_prob=0.1\n",
    "    )\n",
    "    \n",
    "    logger.info(\"Initializing model\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        config.model_name,\n",
    "        config=model_config,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    if num_added_tokens > 0:\n",
    "        logger.info(\"Resizing token embeddings\")\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "        logger.info(f\"Token embeddings resized to {len(tokenizer)}\")\n",
    "    \n",
    "    if hasattr(model, 'classifier'):\n",
    "        logger.info(\"Initializing classifier layer\")\n",
    "        model.classifier.weight.data.normal_(\n",
    "            mean=0.0, \n",
    "            std=model.config.initializer_range if hasattr(model.config, 'initializer_range') else 0.02\n",
    "        )\n",
    "        model.classifier.bias.data.zero_()\n",
    "    \n",
    "    logger.info(\"Initializing data processor\")\n",
    "    processor = TACREDProcessor(tokenizer, config.max_length)\n",
    "    \n",
    "    logger.info(\"Creating datasets...\")\n",
    "    def create_dataset_with_progress(data, desc):\n",
    "        processed_examples = []\n",
    "        for example in tqdm(data, desc=f\"Processing {desc}\"):\n",
    "            try:\n",
    "                processed = processor.process_example(example)\n",
    "                processed_examples.append(processed)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing example in {desc}: {e}\")\n",
    "                continue\n",
    "        return PreprocessedTACREDDataset(processed_examples)\n",
    "    \n",
    "    train_dataset = create_dataset_with_progress(train_data, \"train dataset\")\n",
    "    eval_dataset = create_dataset_with_progress(val_data, \"validation dataset\")\n",
    "    test_dataset = create_dataset_with_progress(test_data, \"test dataset\")\n",
    "    logger.info(f\"Created datasets - Train: {len(train_dataset)}, Dev: {len(eval_dataset)}, Test: {len(test_dataset)}\")\n",
    "    \n",
    "    os.makedirs(config.output_dir, exist_ok=True)\n",
    "    logger.info(f\"Created output directory: {config.output_dir}\")\n",
    "    \n",
    "    relation_labels_path = os.path.join(config.output_dir, \"relation_labels.json\")\n",
    "    with open(relation_labels_path, 'w') as f:\n",
    "        json.dump(RELATION_LABELS, f, indent=2)\n",
    "    logger.info(f\"Saved relation labels mapping to {relation_labels_path}\")\n",
    "    \n",
    "    config_path = os.path.join(config.output_dir, \"model_config.json\")\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(vars(config), f, indent=2)\n",
    "    logger.info(f\"Saved model configuration to {config_path}\")\n",
    "    \n",
    "    logger.info(\"Initializing trainer\")\n",
    "    trainer = TACREDTrainer(\n",
    "        config=config,\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset\n",
    "    )\n",
    "\n",
    "    # ---------------------------\n",
    "    # Training Phase\n",
    "    # ---------------------------\n",
    "    # The training phase is commented out so that the professors can run inference\n",
    "    # on pre-trained model without needing to retrain it.\n",
    "    # logger.info(\"Starting training...\")\n",
    "    # try:\n",
    "    #     _, best_f1 = trainer.train()\n",
    "    #     logger.info(f\"Best validation F1: {best_f1:.4f}\")\n",
    "    # except KeyboardInterrupt:\n",
    "    #     logger.info(\"Training interrupted by user\")\n",
    "    # except Exception as e:\n",
    "    #     logger.exception(\"An error occurred during training\")\n",
    "    \n",
    "    best_model_path = os.path.join(config.output_dir, \"best_model\")\n",
    "    if os.path.exists(best_model_path):\n",
    "         logger.info(\"Loading pre-trained model for inference...\")\n",
    "         trainer.model = AutoModelForSequenceClassification.from_pretrained(best_model_path)\n",
    "         trainer.tokenizer = AutoTokenizer.from_pretrained(best_model_path)\n",
    "         trainer.model.to(trainer.device)\n",
    "         # Evaluate on test set\n",
    "         trainer.eval_dataset = test_dataset\n",
    "         test_results = trainer.evaluate()\n",
    "         logger.info(\"Test results:\")\n",
    "         logger.info(f\"Accuracy: {test_results['eval_accuracy']:.4f}\")\n",
    "         logger.info(f\"F1 Score: {test_results['eval_f1']:.4f}\")\n",
    "         logger.info(\"\\nClassification Report:\")\n",
    "         logger.info(test_results['classification_report'])\n",
    "    else:\n",
    "         logger.warning(f\"No pre-trained model found at {best_model_path}.\")\n",
    "    \n",
    "    test_results_path = os.path.join(config.output_dir, \"test_results.json\")\n",
    "    json_results = {\n",
    "        'eval_loss': float(test_results['eval_loss']) if 'eval_loss' in test_results else None,\n",
    "        'eval_accuracy': float(test_results['eval_accuracy']) if 'eval_accuracy' in test_results else None,\n",
    "        'eval_f1': float(test_results['eval_f1']) if 'eval_f1' in test_results else None,\n",
    "        'classification_report': test_results['classification_report'] if 'classification_report' in test_results else \"\"\n",
    "    }\n",
    "    with open(test_results_path, 'w') as f:\n",
    "        json.dump(json_results, f, indent=2)\n",
    "    logger.info(f\"Saved test results to {test_results_path}\")\n",
    "\n",
    "    logger.info(\"Inference completed successfully\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    log_dir = \"logs\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    log_file = os.path.join(log_dir, f\"tacred_training_{time.strftime('%Y%m%d_%H%M%S')}.log\")\n",
    "    file_handler = logging.FileHandler(log_file)\n",
    "    console_handler = logging.StreamHandler()\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[file_handler, console_handler]\n",
    "    )\n",
    "    logger.info(\"Starting TACRED relation extraction training\")\n",
    "    try:\n",
    "        main()\n",
    "        logger.info(\"Training completed successfully\")\n",
    "    except Exception as e:\n",
    "        logger.exception(\"An error occurred during training\")\n",
    "        raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
